{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asilomar 2022 Signature velocity QC\n",
    "Use thresholds on beam correlation and beam amplitude to discard bad beam velocities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import detrend\n",
    "from datetime import timedelta as TD\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from PyPDF2 import PdfWriter\n",
    "from roxsi_pyfuns.preprocess import signature_preprocess as rpsp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "\n",
    "def natural_sort(l): \n",
    "    \"\"\" \n",
    "    Natural sorting function borrowed from\n",
    "    https://stackoverflow.com/questions/4836710/is-there-a-built-in-function-for-string-natural-sort\n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bathy file\n",
      "Reading atm pressure file\n",
      "Initializing ADCP class\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c595b6ee2f94db185c404417980ddbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-08, datestr:2022-07-07 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0031_1.mat\n",
      "Concatenating datasets for 2022-07-07\n",
      "Saving 2022-07-07 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdce06820f343ce8208f38fbdd9e714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-09, datestr:2022-07-08 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0033_2.mat\n",
      "Concatenating datasets for 2022-07-08\n",
      "Saving 2022-07-08 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001c971afefb4d8dbe1096690fb65e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-10, datestr:2022-07-09 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0035_2.mat\n",
      "Concatenating datasets for 2022-07-09\n",
      "Saving 2022-07-09 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306621d575d94eee9088e4009e62cb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-11, datestr:2022-07-10 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0037_3.mat\n",
      "Concatenating datasets for 2022-07-10\n",
      "Saving 2022-07-10 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48929612972c45bbaa3b9bf4f65d8ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-12, datestr:2022-07-11 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0039_3.mat\n",
      "Concatenating datasets for 2022-07-11\n",
      "Saving 2022-07-11 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f41153974042b0b8e016ea317f6228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-13, datestr:2022-07-12 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0041_3.mat\n",
      "Concatenating datasets for 2022-07-12\n",
      "Saving 2022-07-12 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0f25b90861487c973242074da18b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-14, datestr:2022-07-13 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0043_4.mat\n",
      "Concatenating datasets for 2022-07-13\n",
      "Saving 2022-07-13 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5103a92be04ef3aab355c911f8e94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-15, datestr:2022-07-14 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0045_4.mat\n",
      "Concatenating datasets for 2022-07-14\n",
      "Saving 2022-07-14 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d53f569319d4fc69e967ea4f912acdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-16, datestr:2022-07-15 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0048_1.mat\n",
      "Concatenating datasets for 2022-07-15\n",
      "Saving 2022-07-15 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122fecbbb74e4008acb492f1768a000f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-17, datestr:2022-07-16 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0050_1.mat\n",
      "Concatenating datasets for 2022-07-16\n",
      "Saving 2022-07-16 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfe8ab48f5b4fa295f0d4bfc7270222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-18, datestr:2022-07-17 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0052_1.mat\n",
      "Concatenating datasets for 2022-07-17\n",
      "Saving 2022-07-17 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35566f81bee45698808053ec4ec17aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-19, datestr:2022-07-18 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0054_2.mat\n",
      "Concatenating datasets for 2022-07-18\n",
      "Saving 2022-07-18 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c1c38234a04477a760a484875be3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-20, datestr:2022-07-19 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0056_2.mat\n",
      "Concatenating datasets for 2022-07-19\n",
      "Saving 2022-07-19 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8386cf69acf40a9a2699fff617e2db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to next date. date0: 2022-07-21, datestr:2022-07-20 ...\n",
      "fn: /media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/Signatures/raw/103110/S103110A003_AS22_SS_0058_2.mat\n",
      "Concatenating datasets for 2022-07-20\n",
      "Saving 2022-07-20 serial number 103110 to netCDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbc609ca47f4b14a1f674fae8bf684d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 duplicates found in time_arr\n",
      "duplicates:  [datetime.datetime(2022, 7, 21, 8, 56, 1, 625805)\n",
      " datetime.datetime(2022, 7, 21, 8, 56, 1, 875803)\n",
      " datetime.datetime(2022, 7, 21, 8, 56, 2, 125792)]\n",
      "after removal of dupl.: 0 left\n",
      "3 duplicates found in tb5\n",
      "duplicates:  [datetime.datetime(2022, 7, 21, 8, 56, 1, 813291)\n",
      " datetime.datetime(2022, 7, 21, 8, 56, 2, 63290)\n",
      " datetime.datetime(2022, 7, 21, 8, 56, 2, 313298)]\n",
      "after removal of dupl.: 0 left\n",
      "6 duplicates found in tb5\n",
      "duplicates:  [datetime.datetime(2022, 7, 21, 12, 56, 22, 813395)\n",
      " datetime.datetime(2022, 7, 21, 12, 56, 25, 313301)\n",
      " datetime.datetime(2022, 7, 21, 12, 56, 27, 813499)\n",
      " datetime.datetime(2022, 7, 21, 12, 56, 30, 313395)\n",
      " datetime.datetime(2022, 7, 21, 12, 56, 32, 813401)\n",
      " datetime.datetime(2022, 7, 21, 12, 58, 3, 563402)]\n",
      "after removal of dupl.: 0 left\n",
      "Concatenating datasets for 2022-07-21\n",
      "Saving 2022-07-21 serial number 103110 to netCDF\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Paths etc\n",
    "ser = '103110' # Signature ADCP serial number\n",
    "rootdir = r'/media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray'\n",
    "sigdir = os.path.join(rootdir, 'Signatures')\n",
    "outdir = os.path.join(sigdir, 'Level1', ser)\n",
    "datadir = os.path.join(sigdir, 'raw', ser)\n",
    "fn_minfo = os.path.join(rootdir, 'Asilomar_SSA_2022_mooring_info.xlsx')\n",
    "# Read bathymetry netcdf file\n",
    "bathydir = os.path.join(rootdir, 'Bathy')\n",
    "fn_bathy = os.path.join(bathydir, 'Asilomar_2022_SSA_bathy_updated_50cm.nc')\n",
    "print('Reading bathy file')\n",
    "dsb = xr.decode_cf(xr.open_dataset(fn_bathy, decode_coords='all'))\n",
    "# Atmospheric pressure file\n",
    "fn_patm = os.path.join(rootdir, 'noaa_atm_pressure.csv')\n",
    "print('Reading atm pressure file')\n",
    "dfa = pd.read_csv(fn_patm, parse_dates=['time']).set_index('time')\n",
    "\n",
    "# Initialize ADCP reader class\n",
    "print('Initializing ADCP class')\n",
    "adcp = rpsp.ADCP(datadir=datadir, ser=ser, mooring_info=fn_minfo, \n",
    "                 outdir=outdir, patm=dfa, bathy=None)\n",
    "# Save all datasets for the same date in list for concatenating\n",
    "dsv_daily = [] # Velocities and 1D (eg AST) data\n",
    "dse_daily = [] # Echogram data\n",
    "\n",
    "# Array of dates to be included\n",
    "dates = pd.date_range('2022-06-25', '2022-07-21', freq='1d').strftime('%Y-%m-%d')\n",
    "for datestr in dates:\n",
    "    # print(f'Date: {datestr}')\n",
    "    # Output netcdf filename, check if file already exists\n",
    "    fn_out = os.path.join(outdir, f'sig_{ser}_vel_amp_corr_{pd.Timestamp(datestr).strftime(\"%Y%m%d\")}.nc')\n",
    "    if os.path.isfile(fn_out):\n",
    "        continue\n",
    "    # If netcdf file doesn't exist, read velocities etc. and save to netcdf\n",
    "    ds_list = [] # List for appending daily datasets for concatenating\n",
    "    # Loop over raw .mat files and save daily data as netcdf\n",
    "    for i,fn_mat in tqdm(enumerate(natural_sort(adcp.fns))):\n",
    "        # Check if daily netcdf files already exist\n",
    "        times_mat, times = adcp.read_mat_times(fn_mat=fn_mat)\n",
    "        date0 = str(times[0].date()) # Date of first timestamp\n",
    "        # Check if we can move on to next date\n",
    "        if pd.Timestamp(date0) > pd.Timestamp(datestr):\n",
    "            print(f'Moving on to next date. date0: {date0}, datestr:{datestr} ...')\n",
    "            print(f'fn: {fn_mat}')\n",
    "            break\n",
    "        date1 = str(times[-1].date()) # Date of last timestamp\n",
    "        if date0 != datestr and date1 != datestr:\n",
    "            # print(f'date0={date0}, date1={date1}')\n",
    "            continue\n",
    "        # L5 mat files seem to align exactly with dates,\n",
    "        # so add 1h to date1 if ser == 103206\n",
    "        if ser == '103206':\n",
    "            date1 = str((times[-1] + TD(hours=1)).date())\n",
    "        # Check if date1 is before dataset starttime\n",
    "        if pd.Timestamp(times[-1]) < pd.Timestamp(adcp.t0):\n",
    "            print('.mat file endtime {} before dataset starttime {}'.format(\n",
    "                pd.Timestamp(times[-1]), pd.Timestamp(adcp.t0)))\n",
    "            continue\n",
    "        # Read .mat file to dict\n",
    "        mat = loadmat(fn_mat)\n",
    "        # Read beam vel, amp. and correlation from .mat file into dataset\n",
    "        dsi = adcp.ampcorr2ds(mat)\n",
    "        # Append dataset to list for concatenating\n",
    "        ds_list.append(dsi)\n",
    "    # Concatenate datasets for current date\n",
    "    if len(ds_list) > 0:\n",
    "        print(f'Concatenating datasets for {datestr}')\n",
    "        dsc = xr.concat(ds_list, dim='time')\n",
    "        # Crop dataset to current date only\n",
    "        dsc = dsc.sel(time=datestr)\n",
    "        # Save to netcdf\n",
    "        print(f'Saving {datestr} serial number {ser} to netCDF')\n",
    "        dsc.to_netcdf(fn_out)\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform QC based on threshold on beam correlation and amplitude. Use standard thresholds recommended by Nortek: Correlation threshold = 50%, amplitude threshold = 30 dB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664023658a614b17917d4774eef26136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mids = {'103206':'L5', '103088':'C1', '103094':'C3', '103110':'C6'} # Mooring IDs\n",
    "ser = '103110' # Signature serial number\n",
    "mid = mids[ser]\n",
    "rootdir = r'/media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray'\n",
    "sigdir = os.path.join(rootdir, 'Signatures')\n",
    "outdir = os.path.join(sigdir, 'Level1', ser)\n",
    "datadir = os.path.join(sigdir, 'Level1', ser)\n",
    "corr_thresh = 50 # Beam correlation lower threshold\n",
    "amp_thresh = 30 # Beam amplitude lower threshold\n",
    "dates = pd.date_range('2022-06-25', '2022-07-21', freq='1d').strftime('%Y%m%d')\n",
    "\n",
    "# Iterate over dates and assign amplitude and correlation variables to existing nc files\n",
    "for datestr in tqdm(dates):\n",
    "    # Define new output filename\n",
    "    fn_out = os.path.join(outdir, f'Asilomar_SSA_Sig_Vel_{ser}_{datestr}_v2.nc')\n",
    "    # Check if output file already exists\n",
    "    if os.path.isfile(fn_out):\n",
    "        continue\n",
    "    if datestr == '20220703' and ser == '103088':\n",
    "        continue\n",
    "    # Read new file with ampl + corr.\n",
    "    fn_vac = os.path.join(datadir, f'sig_{ser}_vel_amp_corr_{datestr}.nc')\n",
    "    ds_vac = xr.open_dataset(fn_vac, engine='netcdf4')\n",
    "    # Read original netcdf file w/o ampl + corr\n",
    "    fn_org = os.path.join(datadir, f'Asilomar_SSA_L1_Sig_Vel_{mid}_{datestr}.nc')\n",
    "    ds_org = xr.open_dataset(fn_org, decode_coords='all')\n",
    "    if len(ds_org.time.values) != len(ds_vac.time.values):\n",
    "        # Reindex vac dataset to original time index\n",
    "        # ds_vac = ds_vac.sel(time=slice(ds_org.time.values[0], ds_org.time.values[-1]))\n",
    "        ds_vac = ds_vac.reindex(time=ds_org.time.values, method='nearest', tolerance='10ms')\n",
    "    # Assign variables w/ CF attributes\n",
    "    ds_org['ampB1'] = (['time', 'range'], ds_vac.ab1.values, {'units':'dB', \n",
    "                                                              'standard_name':'signal_intensity_from_multibeam_acoustic_doppler_velocity_sensor_in_sea_water',\n",
    "                                                              'long_name':'Beam 1 amplitude',\n",
    "                                                              },\n",
    "                       )\n",
    "    ds_org['ampB2'] = (['time', 'range'], ds_vac.ab2.values, {'units':'dB', \n",
    "                                                              'standard_name':'signal_intensity_from_multibeam_acoustic_doppler_velocity_sensor_in_sea_water',\n",
    "                                                              'long_name':'Beam 2 amplitude',\n",
    "                                                              },\n",
    "                       )\n",
    "    ds_org['ampB3'] = (['time', 'range'], ds_vac.ab3.values, {'units':'dB', \n",
    "                                                              'standard_name':'signal_intensity_from_multibeam_acoustic_doppler_velocity_sensor_in_sea_water',\n",
    "                                                              'long_name':'Beam 3 amplitude',\n",
    "                                                              },\n",
    "                       )\n",
    "    ds_org['ampB4'] = (['time', 'range'], ds_vac.ab4.values, {'units':'dB', \n",
    "                                                              'standard_name':'signal_intensity_from_multibeam_acoustic_doppler_velocity_sensor_in_sea_water',\n",
    "                                                              'long_name':'Beam 4 amplitude',\n",
    "                                                              },\n",
    "                       )\n",
    "    ds_org['ampB5'] = (['time', 'range'], ds_vac.ab5.values, {'units':'dB', \n",
    "                                                              'standard_name':'signal_intensity_from_multibeam_acoustic_doppler_velocity_sensor_in_sea_water',\n",
    "                                                              'long_name':'Beam 5 amplitude',\n",
    "                                                              },\n",
    "                       )\n",
    "    ds_org['corrB1'] = (['time', 'range'], ds_vac.cb1.values, {'units':'', \n",
    "                                                               'standard_name':'beam_consistency_indicator_from_multibeam_acoustic_doppler_velocity_profiler_in_sea_water',\n",
    "                                                               'long_name':'Beam 1 correlation',\n",
    "                                                               },\n",
    "                       )\n",
    "    ds_org['corrB2'] = (['time', 'range'], ds_vac.cb2.values, {'units':'', \n",
    "                                                               'standard_name':'beam_consistency_indicator_from_multibeam_acoustic_doppler_velocity_profiler_in_sea_water',\n",
    "                                                               'long_name':'Beam 2 correlation',\n",
    "                                                               },\n",
    "                       )\n",
    "    ds_org['corrB3'] = (['time', 'range'], ds_vac.cb3.values, {'units':'', \n",
    "                                                               'standard_name':'beam_consistency_indicator_from_multibeam_acoustic_doppler_velocity_profiler_in_sea_water',\n",
    "                                                               'long_name':'Beam 3 correlation',\n",
    "                                                               },\n",
    "                       )\n",
    "    ds_org['corrB4'] = (['time', 'range'], ds_vac.cb4.values, {'units':'', \n",
    "                                                               'standard_name':'beam_consistency_indicator_from_multibeam_acoustic_doppler_velocity_profiler_in_sea_water',\n",
    "                                                               'long_name':'Beam 4 correlation',\n",
    "                                                               },\n",
    "                       )\n",
    "    ds_org['corrB5'] = (['time', 'range'], ds_vac.cb5.values, {'units':'', \n",
    "                                                               'standard_name':'beam_consistency_indicator_from_multibeam_acoustic_doppler_velocity_profiler_in_sea_water',\n",
    "                                                               'long_name':'Beam 5 correlation',\n",
    "                                                               },\n",
    "                       )\n",
    "    # Save to new netcdf\n",
    "    ds_org.to_netcdf(fn_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot amplitude + correlation + beam vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.close()\n",
    "ser = '103110' # Signature serial number\n",
    "mids = {'103206':'L5', '103088':'C1', '103094':'C3', '103110':'C6'} # Mooring IDs\n",
    "sig_mids = {'C1':'SO', 'C3':'SM', 'C6':'SI', 'L1':'SS', 'L5':'SN'}\n",
    "mid = mids[ser]\n",
    "rootdir = r'/media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray'\n",
    "sigdir = os.path.join(rootdir, 'Signatures')\n",
    "outdir = os.path.join(sigdir, 'Level1', ser)\n",
    "figdir = os.path.join(sigdir, 'Level1', ser, 'qc_fig')\n",
    "if not os.path.isdir(figdir):\n",
    "    os.mkdir(figdir)\n",
    "datadir = os.path.join(sigdir, 'Level1', ser)\n",
    "corr_thresh = 75 # Beam correlation lower threshold\n",
    "amp_thresh = 40 # Beam amplitude lower threshold\n",
    "dates = pd.date_range('2022-07-05', '2022-07-14', freq='1d').strftime('%Y%m%d')\n",
    "\n",
    "for datestr in dates:\n",
    "    # datestr = '20220707'\n",
    "    print(f'Date: {datestr}')\n",
    "    # Read new netcdf file\n",
    "    fn = os.path.join(datadir, f'Asilomar_SSA_Sig_Vel_{ser}_{datestr}_v2.nc')\n",
    "    ds = xr.open_dataset(fn, decode_coords='all')\n",
    "\n",
    "    # Iterate over hours and bins and estimate beam velocity spectra\n",
    "    hours = pd.date_range(pd.Timestamp(datestr), pd.Timestamp(datestr)+pd.Timedelta(days=1), freq='1h')\n",
    "    for t0,t1 in tqdm(zip(hours[:-1], hours[1:])):\n",
    "        # Take out hourly segment\n",
    "        seg = ds.sel(time=slice(t0,t1)).copy()\n",
    "        # Make masks for keeping only good values\n",
    "        valid = (seg.range < (0.9 * seg.ASTd.min(dim='time').item())) # Valid range values below surface\n",
    "        good = {'B1': ((seg.ampB1>amp_thresh) & (seg.corrB1>corr_thresh)), # Good Beam1 values based on amp&corr\n",
    "                'B2': ((seg.ampB2>amp_thresh) & (seg.corrB2>corr_thresh)), # Good Beam2 values based on amp&corr\n",
    "                'B3': ((seg.ampB3>amp_thresh) & (seg.corrB3>corr_thresh)), # Good Beam3 values based on amp&corr\n",
    "                'B4': ((seg.ampB4>amp_thresh) & (seg.corrB4>corr_thresh)), # Good Beam4 values based on amp&corr\n",
    "                'B5': ((seg.ampB5>amp_thresh) & (seg.corrB5>corr_thresh)), # Good Beam5 values based on amp&corr\n",
    "                }\n",
    "        # np.isnan(ds.vB1.where((ds.ampB1>40) & (ds.corrB1>50))).plot(ax=ax, x='time', y='range')\n",
    "        for b in ['vB1', 'vB2', 'vB3', 'vB4', 'vB5']:\n",
    "            # Filename for merged pdf\n",
    "            fn_merged = os.path.join(figdir, f'merged_{ser}_{b}_{datestr}.pdf')\n",
    "            if os.path.isfile(fn_merged):\n",
    "                continue\n",
    "            # Output figure filename\n",
    "            fn_fig = os.path.join(figdir, f'{ser}_{b}_{t0.strftime(\"%Y%m%d_%H%M\")}.pdf')\n",
    "            if os.path.isfile(fn_fig):\n",
    "                continue\n",
    "            fig, axes = plt.subplots(figsize=(12,6), nrows=2, constrained_layout=True, sharex=True)\n",
    "            # Select only valid data\n",
    "            good_vb = seg[b].where((valid & good['B1'] & good['B2'] & good['B3'] & good['B4'])).copy()\n",
    "            # Only use range bins with less than 10% dropouts/discarded values\n",
    "            good_ranges = (np.isnan(good_vb).sum(axis=0) < len(good_vb.time.values)*0.1)\n",
    "            # Check if any good range bins, else skip hour\n",
    "            if np.sum(good_ranges) == 0:\n",
    "                continue\n",
    "            good_vb = good_vb.sel(range=(good_ranges.values))\n",
    "            # Plot beam velocity profile on top (downsample for plot)\n",
    "            good_vb.resample(time='2s').mean().plot(ax=axes[0], x='time', y='range', cbar_kwargs={\"pad\":0.005})\n",
    "            # Also plot AST range\n",
    "            AST_mean = seg.ASTd.mean(dim='time').item()\n",
    "            AST_dt = detrend(seg.ASTd.interpolate_na(dim='time').bfill(dim='time').ffill(dim='time'))\n",
    "            axes[0].plot(seg.time, (AST_dt+AST_mean), c='k')\n",
    "            # Timeseries at different range bins\n",
    "            if ser == '103094':\n",
    "                ranges = [3.5, 2.25, 1]\n",
    "            else:\n",
    "                ranges = [5.5, 3.5, 1.5]\n",
    "            cs = ['k', 'C0', 'C1'] # Colors\n",
    "            for ri,r in enumerate(ranges):\n",
    "                axes[0].axhline(r, ls='--', lw=0.75, c=cs[ri], label=f'range={r}m')\n",
    "                timeseries = good_vb.sel(range=r, method='nearest').interpolate_na(dim='time').copy()\n",
    "                (timeseries+0.35).plot(ax=axes[1], alpha=0.75, c=cs[ri], label=f'range={r}m',)\n",
    "            axes[0].set_xlabel(None)\n",
    "            axes[0].set_ylim([0, seg.ASTd.max()+0.25])\n",
    "            axes[0].set_title(f'{ser} Signature ({sig_mids[mid]}) Beam {b[-1]} velocity for {t0}-{t1.time()}')\n",
    "            axes[1].set_title(None)\n",
    "            axes[1].legend(ncols=len(ranges))\n",
    "            # plt.show()\n",
    "            # Save fig\n",
    "            plt.savefig(fn_fig, bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    # Merge individual pdfs into one\n",
    "    for b in ['vB1', 'vB2', 'vB3', 'vB4', 'vB5']:\n",
    "        # Filename for merged pdf\n",
    "        fn_merged = os.path.join(figdir, f'merged_{ser}_{b}_{datestr}.pdf')\n",
    "        if os.path.isfile(fn_merged):\n",
    "            continue\n",
    "        # List all pdf files for current beam\n",
    "        fns_pdf = sorted(glob.glob(os.path.join(figdir, f'{ser}_{b}_*.pdf')))\n",
    "        # Initialize pdf merging class\n",
    "        merger = PdfWriter()\n",
    "        # print('Merging individual pdf files ...')\n",
    "        # Append individual pdf files\n",
    "        for pdf in fns_pdf:\n",
    "            merger.append(pdf)\n",
    "\n",
    "        merger.write(fn_merged)\n",
    "        merger.close()\n",
    "        # Remove individual pdf files\n",
    "        # print('Removing individual pdf files ...')\n",
    "        for fn in fns_pdf:\n",
    "            os.remove(fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare ADCP beam velocity statistics to linear wave theory predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roxsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
