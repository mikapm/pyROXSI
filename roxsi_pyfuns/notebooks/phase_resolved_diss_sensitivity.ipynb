{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity of phase-resolved $\\epsilon$ to tilt angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import optimize\n",
    "from scipy.signal import detrend\n",
    "import statsmodels.api as sm\n",
    "import cmocean\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.ticker as tck\n",
    "from roxsi_pyfuns.plotting import multiple_formatter\n",
    "from datetime import datetime as DT\n",
    "from cftime import date2num, num2date\n",
    "# Interactive plots\n",
    "%matplotlib widget \n",
    "\n",
    "from roxsi_pyfuns import coordinate_transforms as rpct\n",
    "from roxsi_pyfuns import wave_spectra as rpws\n",
    "from roxsi_pyfuns import zero_crossings as rpzc\n",
    "from roxsi_pyfuns import stats as rps\n",
    "from roxsi_pyfuns import turbulence as rpt\n",
    "from roxsi_pyfuns import plotting as rppl\n",
    "\n",
    "# Paths\n",
    "rootdir = r'/media/mikapm/T7 Shield/ROXSI/Asilomar2022/SmallScaleArray/' # For laptop\n",
    "# rootdir = r'/home/malila/ROXSI/Asilomar2022/SmallScaleArray/' # For PC\n",
    "vec_root = os.path.join(rootdir, 'Vectors', 'Level1')\n",
    "figdir = os.path.join(vec_root, 'img')\n",
    "# Bathymetry\n",
    "bathydir = os.path.join(rootdir, 'Bathy')\n",
    "fn_bathy = os.path.join(bathydir, 'Asilomar_2022_SSA_bathy_updated_1m.nc')\n",
    "dsbat = xr.decode_cf(xr.open_dataset(fn_bathy, decode_coords='all'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mids = ['C2', 'C3', 'L1']\n",
    "sers = ['17212', '17219', '6338']\n",
    "# Tilt angle offsets\n",
    "roll_offs = 1\n",
    "pitch_offs = 0\n",
    "# Bursts to skip table\n",
    "mstr = ''.join(mids) # String joining all mooring IDs \n",
    "if mstr == 'C2C3C4':\n",
    "    fn_skip = os.path.join(rootdir, f'vec_bursts_skip_{mstr}.xlsx')\n",
    "    dfsk = pd.read_excel(fn_skip, parse_dates=['time'])\n",
    "# mids = ['C3', 'L1']\n",
    "# sers = ['17219', '6338']\n",
    "fs = 16 # Sampling freq\n",
    "# Define number of sub-segments per wave cycle (should be even)\n",
    "ncyc = 8\n",
    "# Minimum R^2 threshold for inertial subrange fits\n",
    "r2_thresh = 0.9\n",
    "# Minimum ratio of eddy timescale over advection timescale\n",
    "eddy_adv_ratio = 5\n",
    "# Sample period\n",
    "# t0 = pd.Timestamp('2022-07-14 14:00:00')\n",
    "t0 = pd.Timestamp('2022-07-08 00:00:00')\n",
    "# t0 = pd.Timestamp('2022-07-15 00:00:00')\n",
    "nhours = 1 # No. of hours to process at once\n",
    "t1 = t0 + pd.Timedelta(hours=nhours)\n",
    "spd = 60 * 60 * 24 # Seconds per day\n",
    "# Define expected headings in PCA coordinates for C2, C3 & C4\n",
    "heading_exp_1 = {'C2': -125, 'C3': -90, 'C4': -90, 'L2': None, 'L1': 160}\n",
    "# After 2022-07-13 04:00 (C4)\n",
    "heading_exp_2 = {'C2': -125, 'C3': -90, 'C4': -20, 'L2': None, 'L1': 160}\n",
    "# Maximum allowable fraction of spikes in vertical vel.\n",
    "max_spikes_frac = 0.2\n",
    "\n",
    "# Dataframes for burst-average dissipation and U_rms\n",
    "time_range = pd.date_range(t0, t1, freq='1H') # Time index\n",
    "nt = len(time_range)\n",
    "dfbd = {'{}'.format(m): pd.DataFrame(data={'eps_phase': np.ones(nt)*np.nan, \n",
    "                                           'eps_phase_std': np.ones(nt)*np.nan,\n",
    "                                           'eps_LT83': np.ones(nt)*np.nan,\n",
    "                                           'U_rms': np.ones(nt)*np.nan,\n",
    "                                           'depth': np.ones(nt)*np.nan,\n",
    "                                           },\n",
    "                                     index=time_range) for m in mids\n",
    "        }\n",
    "# Dissipation rates per wave-cycle subsegment\n",
    "diss_subseg = {'{}'.format(m): {} for m in mids}\n",
    "U_subseg = {'{}'.format(m): {} for m in mids}\n",
    "for mid in mids:\n",
    "    diss_subseg[mid] = {'{}/{}'.format(n, ncyc): [] for n in range(1,ncyc+1)}\n",
    "    U_subseg[mid] = {'{}/{}'.format(n, ncyc): [] for n in range(1,ncyc+1)}\n",
    "# Wave-by-wave dissipation rates lists for concatenating dataframes\n",
    "dfw_list = {'{}'.format(m): [] for m in mids}\n",
    "\n",
    "# Lists for histograms\n",
    "udu_ratios = {} # U/dU ratios\n",
    "t_ratios = {} # eddy/adv time scale ratios\n",
    "for mid in mids:\n",
    "    udu_ratios[mid] = []\n",
    "    t_ratios[mid] = []\n",
    "# List for timestamps of bursts to skip due to too many spikes\n",
    "skip_list = []\n",
    "\n",
    "# Define type of fit to use for dissipation rate calculation\n",
    "fit = 'linear'\n",
    "# Define number of bins to use for advection speed U (1-4)\n",
    "nbins = 4\n",
    "if nbins == 2:\n",
    "    # Define U ranges for 2 bins\n",
    "    ubins = [(0, 0.5), (0.5, 2)]\n",
    "elif nbins == 3:\n",
    "    # Define U ranges for 3 bins\n",
    "    ubins = [(0, 0.25), (0.25, 0.5), (0.5, 2)]\n",
    "elif nbins == 4:\n",
    "    # Define U ranges for 4 bins\n",
    "    ubins = [(0, 0.15), (0.15, 0.3), (0.3, 0.45), (0.45, 2)]\n",
    "\n",
    "# Iterate over requested moorings, figure out which bursts to skip\n",
    "print('Finding bursts to skip ...')\n",
    "for mid, ser in zip(mids, sers):\n",
    "    print('{} - {}'.format(mid, ser))\n",
    "    # Read netcdf file, check if need to concatenate multiple days\n",
    "    if ((t1.date() - t0.date()).total_seconds() / spd) > 0:\n",
    "        # More than one day -> concatenate daily datasets\n",
    "        drc = pd.date_range(t0.date(), t1.date(), freq='1D') # Range of dates\n",
    "        dlc = [] # List for concatenating\n",
    "        for d in drc:\n",
    "            ncdir = os.path.join(vec_root, mid)\n",
    "            fn_vec = os.path.join(ncdir, 'Asilomar_SSA_L1_Vec_{}_{}.nc'.format(\n",
    "                ser, str(d.strftime('%Y%m%d'))))\n",
    "            dsv = xr.decode_cf(xr.open_dataset(fn_vec, decode_coords='all'))\n",
    "            # Append to list\n",
    "            dlc.append(dsv)\n",
    "        # Concatenate daily datasets\n",
    "        dsv = xr.concat(dlc, dim='time')\n",
    "    else:\n",
    "        # Only one day\n",
    "        ncdir = os.path.join(vec_root, mid)\n",
    "        fn_vec = os.path.join(ncdir, 'Asilomar_SSA_L1_Vec_{}_{}.nc'.format(ser, \n",
    "            str(t0.strftime('%Y%m%d'))))\n",
    "        dsv = xr.decode_cf(xr.open_dataset(fn_vec, decode_coords='all'))\n",
    "\n",
    "    # Iterate over bursts\n",
    "    for t0b in time_range:\n",
    "        t1b = t0b + pd.Timedelta(hours=1)\n",
    "        if t1b > t1:\n",
    "            continue\n",
    "        # Check bursts to skip based on non-stationary tilt angles or spikes\n",
    "        if mstr == 'C2C3C4' and t0b in dfsk.values:\n",
    "            # Bad tilt angles in one of the C2/C3/C4 Vectors\n",
    "            continue\n",
    "        # Select sample period\n",
    "        seg = dsv.sel(time=slice(t0b, t1b))\n",
    "        # Check if too many spikes\n",
    "        spikes_frac = np.sum(~np.isclose(seg.uz, seg.uzd)) / len(seg.uz)\n",
    "        if spikes_frac > max_spikes_frac:\n",
    "            # Too many spikes - skip burst\n",
    "            print(f'Spike fraction {spikes_frac:.2f} > {max_spikes_frac} -> skipping')\n",
    "            skip_list.append(t0b) # Append burst start time to skip list\n",
    "\n",
    "# Main loop to iterate over desired moorings\n",
    "print('Main loop ...')\n",
    "for mid, ser in zip(mids, sers):\n",
    "    print('{} - {}'.format(mid, ser))\n",
    "    # Read netcdf file, check if need to concatenate multiple days\n",
    "    if ((t1.date() - t0.date()).total_seconds() / spd) > 0:\n",
    "        # More than one day -> concatenate daily datasets\n",
    "        drc = pd.date_range(t0.date(), t1.date(), freq='1D') # Range of dates\n",
    "        dlc = [] # List for concatenating\n",
    "        for d in drc:\n",
    "            ncdir = os.path.join(vec_root, mid)\n",
    "            fn_vec = os.path.join(ncdir, 'Asilomar_SSA_L1_Vec_{}_{}.nc'.format(\n",
    "                ser, str(d.strftime('%Y%m%d'))))\n",
    "            dsv = xr.decode_cf(xr.open_dataset(fn_vec, decode_coords='all'))\n",
    "            # Append to list\n",
    "            dlc.append(dsv)\n",
    "        # Concatenate daily datasets\n",
    "        dsv = xr.concat(dlc, dim='time')\n",
    "    else:\n",
    "        # Only one day\n",
    "        ncdir = os.path.join(vec_root, mid)\n",
    "        fn_vec = os.path.join(ncdir, 'Asilomar_SSA_L1_Vec_{}_{}.nc'.format(ser, \n",
    "            str(t0.strftime('%Y%m%d'))))\n",
    "        dsv = xr.decode_cf(xr.open_dataset(fn_vec, decode_coords='all'))\n",
    "\n",
    "    # Filename of burst diss. rate csv file (for sensitivity testing)\n",
    "    csvdir = os.path.join(ncdir, 'dissipation_rate', 'sensitivity_tests')\n",
    "    if not os.path.isdir(csvdir):\n",
    "        os.mkdir(csvdir)\n",
    "    fn_csv_diss = os.path.join(csvdir, 'burst_diss_{}_{}_nhours_{}_ro_{}_po_{}.csv'.format(\n",
    "        mid, t0.strftime('%Y%m%d_%H%M'), nhours, roll_offs, pitch_offs))\n",
    "    if not os.path.isfile(fn_csv_diss):\n",
    "        # Wave counter\n",
    "        wcnt = 0\n",
    "        # Iterate over bursts\n",
    "        for t0b in time_range:\n",
    "            t1b = t0b + pd.Timedelta(hours=1)\n",
    "            if t1b > t1:\n",
    "                continue\n",
    "            # Check bursts to skip based on non-stationary tilt angles or spikes\n",
    "            if mstr == 'C2C3C4' and t0b in dfsk.values:\n",
    "                # Bad tilt angles in one of the C2/C3/C4 Vectors\n",
    "                continue\n",
    "            if t0b in skip_list:\n",
    "                # Too many spikes in another Vector burst\n",
    "                print(f'Too many spikes in {t0b}')\n",
    "                continue\n",
    "            # print('{}-{}'.format(t0b, t1b.time()))\n",
    "            # Select sample period\n",
    "            seg = dsv.sel(time=slice(t0b, t1b))\n",
    "            # Get sea-surface elevation (K_rms transform)\n",
    "            eta = seg.eta_lin_krms.copy().to_series() # Surface elevation\n",
    "            # Convert velocities to cross-/alongshore\n",
    "            if mid in ['C4', 'L1', 'L5', 'C1', 'C6']:\n",
    "                # x vel, despiked\n",
    "                uxd = seg.uxd.to_dataframe() # Convert to pandas\n",
    "                uxd = uxd.interpolate(method='bfill').interpolate('ffill')\n",
    "                uxm = uxd.mean().item()\n",
    "                uxd -= uxm\n",
    "                # y vel, despiked\n",
    "                uyd = seg.uyd.to_dataframe() # Convert to pandas\n",
    "                uyd = uyd.interpolate(method='bfill').interpolate('ffill')\n",
    "                uym = uyd.mean().item()\n",
    "                uyd -= uym\n",
    "                # z vel, despiked\n",
    "                uzd = seg.uzd.to_dataframe() # Convert to pandas\n",
    "                uzd = uzd.interpolate(method='bfill').interpolate('ffill')\n",
    "                uzd -= uzd.mean()\n",
    "                # Get correct expected heading for C4\n",
    "                if t0b <= pd.Timestamp('2022-07-13 04:00'):\n",
    "                    he = heading_exp_1[mid]\n",
    "                else:\n",
    "                    he = heading_exp_2[mid]\n",
    "                # Rotate velocities to cross/alongshore & vertical using PCA\n",
    "                ucs, uls, uw = rpct.enu_to_loc_pca(ux=uxd.values.squeeze(), \n",
    "                                                   uy=uyd.values.squeeze(), \n",
    "                                                   uz=uzd.values.squeeze(),\n",
    "                                                   heading_exp=he, \n",
    "                                                   # print_msg=True,\n",
    "                                                   )\n",
    "            else:\n",
    "                # Convert E,N velocities to local cross- & alongshore (x,y) components\n",
    "                uE = seg.uE.to_dataframe() # Convert to pandas\n",
    "                uE = uE.interpolate(method='bfill').interpolate('ffill')\n",
    "                uN = seg.uN.to_dataframe() # Convert to pandas\n",
    "                uN = uN.interpolate(method='bfill').interpolate('ffill')\n",
    "                uU = seg.uU.to_dataframe() # Convert to pandas\n",
    "                uU = uU.interpolate(method='bfill').interpolate('ffill')\n",
    "                dss_seg = rpws.spec_uvz(z=eta.values, \n",
    "                                        u=uE.values.squeeze(), \n",
    "                                        v=uN.values.squeeze(), \n",
    "                                        fs=16,\n",
    "                                        )\n",
    "#                 if mid == 'C4':\n",
    "#                     # Get correct expected heading for C4\n",
    "#                     if t0b <= pd.Timestamp('2022-07-13 04:00'):\n",
    "#                         he = heading_exp_1[mid]\n",
    "#                     else:\n",
    "#                         he = heading_exp_2[mid]\n",
    "#                     # Rotate w/ PCA (test)\n",
    "#                     ucs, uls, uw = rpct.enu_to_loc_pca(ux=uE.values.squeeze(), \n",
    "#                                                        uy=uN.values.squeeze(), \n",
    "#                                                        uz=uU.values.squeeze(),\n",
    "#                                                        heading_exp=None, \n",
    "#                                                        # print_msg=True,\n",
    "#                                                        )\n",
    "#                else:\n",
    "                angle_met = dss_seg.mdir.item() # Cross-shore angle\n",
    "                angle_math = 270 - angle_met # Math angle to rotate\n",
    "                if angle_math < 0:\n",
    "                    angle_math += 360\n",
    "                angle_math = np.deg2rad(angle_math) # Radians\n",
    "                # Rotate East and North velocities to cross-shore (cs) and \n",
    "                # long-shore (ls)\n",
    "                ucs, uls = rpct.rotate_vel(uE.values.squeeze(), \n",
    "                                            uN.values.squeeze(), \n",
    "                                            angle_math,\n",
    "                                            )\n",
    "                uw = uU.values.squeeze() # Vertical vel.\n",
    "            # Estimate U-rms for burst\n",
    "            uspec = rpws.spec_uvz(ucs, fs=16, fmerge=5)\n",
    "            vspec = rpws.spec_uvz(uls, fs=16, fmerge=5)\n",
    "            # Variance of cross- and alongshore orbital velocities\n",
    "            m0u = rpws.spec_moment(uspec.Ezz.values, uspec.freq.values, 0)\n",
    "            m0v = rpws.spec_moment(vspec.Ezz.values, vspec.freq.values, 0)\n",
    "            Urms = np.sqrt(2 * (m0u + m0v))\n",
    "            # Mean current\n",
    "            Uburst = np.sqrt(uxm**2 + uym**2)\n",
    "            # Mean depth\n",
    "            h = seg.z_hyd.mean().item()\n",
    "            # Estimate full w-spectrum (testing)\n",
    "            wspec = rpws.spec_uvz(uw, fs=16, fmerge=5)\n",
    "\n",
    "            # Estimate full burst dissipation rate following \n",
    "            # Lumley & Terray (1983) and Trowbridge & Elgar (2001)\n",
    "            eps, r2, cf = rpt.dissipation_rate_LT83(f=wspec.freq.values.squeeze(),\n",
    "                                                    spec=wspec.Ezz.values.squeeze(),\n",
    "                                                    U=Uburst, sigma=np.sqrt(m0u+m0v),\n",
    "                                                    skip_f=10)\n",
    "            eps_lt83[mid].append(eps)\n",
    "            dfbd[mid]['eps_LT83'].loc[t0b] = eps\n",
    "            # dfbd83[mid].loc[t0b] = eps\n",
    "\n",
    "            # Get zero-crossings, min. 4-sec waves\n",
    "            zc, Hw, Hc, Ht = rpzc.get_waveheights(eta.values, method='up', minlen=4*16)\n",
    "            nwaves = len(Hw)\n",
    "            # Compute H(1/3)\n",
    "            Hws = np.sort(Hw) # Sorted wave heights\n",
    "            H13 = np.mean(Hws[2*(nwaves//3):]) # Mean of highest 1/3 of waves\n",
    "            # Define phase (x) axis to interpolate to\n",
    "            n_phase = 160 # Number of points in phase axis\n",
    "            min_period = 6 # Min. wave period (sec) to include\n",
    "\n",
    "            # Lists for storing dataframes of spectral segments for merging\n",
    "            # for different ranges of advection speed U\n",
    "            dfsi_lists_1 = {'dfs{}'.format(n): [] for n in range(1,ncyc+1)}\n",
    "            dfsi_lists_2 = {'dfs{}'.format(n): [] for n in range(1,ncyc+1)}\n",
    "            dfsi_lists_3 = {'dfs{}'.format(n): [] for n in range(1,ncyc+1)}\n",
    "            dfsi_lists_4 = {'dfs{}'.format(n): [] for n in range(1,ncyc+1)}\n",
    "            dssi = {'q{}'.format(n): [] for n in range(1,ncyc+1)}\n",
    "            # Check if spectral datasets saved to netcdf\n",
    "            spec_ncdir = os.path.join(ncdir, 'k_specs')\n",
    "            if not os.path.isdir(spec_ncdir):\n",
    "                os.mkdir(spec_ncdir)\n",
    "            csvdir_phase = os.path.join(ncdir, 'phase_res_int')\n",
    "            if not os.path.isdir(csvdir_phase):\n",
    "                os.mkdir(csvdir_phase)\n",
    "            # Interpolate eta, u_cs and w for each wave to wave phase (0-2*pi)\n",
    "            for zi,zc0 in enumerate(zc[:-1]):\n",
    "                # Only include waves with T > min_period sec\n",
    "                Tw = (eta.index[zc[zi+1]] - eta.index[zc[zi]]).total_seconds()\n",
    "                if Tw < min_period:\n",
    "                    continue\n",
    "                # Also only include high waves\n",
    "                Hwi = Hw[zi]\n",
    "    #             if Hwi < H13:\n",
    "    #                 continue\n",
    "                # Wave surface elevation, u, v and w for current wave\n",
    "                eta_wave = eta.iloc[zc0:zc[zi+1]]\n",
    "                u_wave = ucs[zc0:zc[zi+1]]\n",
    "                v_wave = uls[zc0:zc[zi+1]]\n",
    "                w_wave = uw[zc0:zc[zi+1]]\n",
    "                # Surface elevation interpolation\n",
    "                df_e = rpzc.interpolate_phase(eta_wave.squeeze(), N=n_phase, \n",
    "                                              label='eta_int_{:04d}'.format(wcnt))\n",
    "                wave_int_e[mid].append(df_e)\n",
    "                # Save to csv\n",
    "                fn_csv_dfe = os.path.join(csvdir_phase, \n",
    "                                          'e_int_{}_{}_{:04d}.csv'.format(\n",
    "                                              mid, t0b.strftime('%Y%m%d_%H%M'), wcnt))\n",
    "#                 if not os.path.isfile(fn_csv_dfe):\n",
    "#                     df_e.to_csv(fn_csv_dfe)\n",
    "                # Cross-shore vel. interpolation\n",
    "                df_u = rpzc.interpolate_phase(u_wave.squeeze(), N=n_phase, \n",
    "                                            label='u_int_{:04d}'.format(wcnt))\n",
    "                wave_int_u[mid].append(df_u)\n",
    "                # Save to csv\n",
    "                fn_csv_dfu = os.path.join(csvdir_phase, \n",
    "                                          'u_int_{}_{}_{:04d}.csv'.format(\n",
    "                                              mid, t0b.strftime('%Y%m%d_%H%M'), wcnt))\n",
    "#                 if not os.path.isfile(fn_csv_dfu):\n",
    "#                     df_u.to_csv(fn_csv_dfu)\n",
    "                # Long-shore vel. interpolation\n",
    "                df_v = rpzc.interpolate_phase(v_wave.squeeze(), N=n_phase, \n",
    "                                            label='u_int_{:04d}'.format(wcnt))\n",
    "                wave_int_v[mid].append(df_v)\n",
    "                # Save to csv\n",
    "                fn_csv_dfv = os.path.join(csvdir_phase, \n",
    "                                          'v_int_{}_{}_{:04d}.csv'.format(\n",
    "                                              mid, t0b.strftime('%Y%m%d_%H%M'), wcnt))\n",
    "#                 if not os.path.isfile(fn_csv_dfv):\n",
    "#                     df_v.to_csv(fn_csv_dfv)\n",
    "                # Vertical vel. interpolation\n",
    "                df_w = rpzc.interpolate_phase(w_wave.squeeze(), N=n_phase, \n",
    "                                            label='u_int_{:04d}'.format(wcnt))\n",
    "                wave_int_w[mid].append(df_w)\n",
    "                # Save to csv\n",
    "                fn_csv_dfw = os.path.join(csvdir_phase, \n",
    "                                          'w_int_{}_{}_{:04d}.csv'.format(\n",
    "                                              mid, t0b.strftime('%Y%m%d_%H%M'), wcnt))\n",
    "#                 if not os.path.isfile(fn_csv_dfw):\n",
    "#                     df_w.to_csv(fn_csv_dfw)\n",
    "                # Also make df of non-interpolated vert. vel. with phase axis\n",
    "                phase_ni = np.linspace(0, 1, len(w_wave)) #* 2*np.pi # non-int. phases\n",
    "                df_wni = pd.DataFrame(data=w_wave.squeeze(), index=phase_ni)\n",
    "                # Save to csv\n",
    "                fn_csv_dfwni = os.path.join(csvdir_phase, \n",
    "                                            'w_nonint_{}_{}_{:04d}.csv'.format(\n",
    "                                                mid, t0b.strftime('%Y%m%d_%H%M'), wcnt))\n",
    "#                 if not os.path.isfile(fn_csv_dfwni):\n",
    "#                     df_wni.to_csv(fn_csv_dfwni)\n",
    "                # Increase wave counter\n",
    "                wcnt += 1\n",
    "\n",
    "                # Estimate spectra for 1/ncyc wave cycles\n",
    "                nsub = n_phase // ncyc # no. of samples per phase sub-segment\n",
    "                nsub_ni = len(phase_ni) // ncyc # non-interpolated # of samples\n",
    "                # Frequencies\n",
    "                n_freqs = int(np.floor(nsub / 2 )) # No. of frequency bands\n",
    "                n_freqs_ni = int(np.floor(nsub_ni / 2 )) # No. of frequency bands\n",
    "                # Wavenumbers to interpolate spectrum to\n",
    "                n_int = 30 # Number of wavenumbers to interpolate to\n",
    "                kmin = (2*np.pi) / 0.5 # Min. interpolation wavenumber\n",
    "                kmax = (2*np.pi) / 0.05 # Max. interpolation wavenumber\n",
    "                ki = np.linspace(kmin, kmax, n_int) # Interp. target k's\n",
    "                # Initialize dict to store spectral dataframes (incl. U)\n",
    "                dfsi = {} # k-spec dataframes\n",
    "                for nb in range(1, nbins+1):\n",
    "                    dfsi['{}'.format(nb)] = {}\n",
    "                for i in range(ncyc):\n",
    "                    # Get short-segment interpolated velocities\n",
    "                    useg = df_u.iloc[i*nsub:(i+1)*nsub].values.squeeze()\n",
    "                    vseg = df_v.iloc[i*nsub:(i+1)*nsub].values.squeeze()\n",
    "                    wseg = df_w.iloc[i*nsub:(i+1)*nsub].values.squeeze()\n",
    "                    # Also get non-interpolated vertical velocity segment\n",
    "                    wseg_ni = df_wni.iloc[i*nsub_ni:(i+1)*nsub_ni].values.squeeze()\n",
    "                    # Timestamp\n",
    "                    t0seg = (pd.Timestamp(eta_wave.index[0]) + \n",
    "                            pd.Timedelta(seconds=(Tw/n_int)*(i/ncyc)))\n",
    "                    # Compute mean current and check that U >> dU for segment\n",
    "                    U = np.sqrt(np.mean(useg)**2 + np.mean(vseg)**2).item() # Mean orb. vel. mag.\n",
    "                    umag = np.sqrt(useg**2 + vseg**2) # Orb. vel. magnitude, not mean\n",
    "                    dU = np.std(umag) # Variability in umag (following George et al., 1994)\n",
    "                    # Append U / dU ratio to list for histogram\n",
    "                    udu_ratios[mid].append(U / dU)\n",
    "                    # Check U/dU ratio\n",
    "                    if (U / dU) < 1:\n",
    "                        # U vs. dU ratio (Eq (6) of George et al., 1994) too small\n",
    "                        # Use U / dU = 1 as threshold following Rosman & Gerbi (2017), \n",
    "                        # Fig. 9\n",
    "                        continue\n",
    "                    # Check that the turnover timescale of the eddies is much longer \n",
    "                    # than the time for advection of the eddies past the sensor.\n",
    "                    # First estimate k spectrum of w following George et al. (1994)\n",
    "                    dsk_i = rpt.k_spec_wavephase(w=wseg_ni, U=U)\n",
    "                    # Check highest allowable k based on sampling rate and volume\n",
    "                    maxk_check = np.min((kmax, np.pi*fs/U))\n",
    "                    # Set to NaN all spectral elements with k > maxk\n",
    "                    dsk_i.k_spec.values = dsk_i.k_spec.where(dsk_i.k < maxk_check).values\n",
    "                    # Make (noisy) first estimate of dissipation rate from single\n",
    "                    # spectrum for eddy time scale\n",
    "                    rs_list = [] # R^2 values of fits\n",
    "                    eps_list = [] # Dissipation rate estimates from fits\n",
    "                    si_list = [] # Start indices of fits (to get lowest k of best fit)\n",
    "                    sis = np.arange(1, n_freqs_ni//2) # Start indices of -5/3 fit\n",
    "                    eis = -np.arange(1, n_freqs_ni//2)[::-1] # End indices of fit\n",
    "                    # Iterate over different k ranges and fit inertial subrange\n",
    "                    for si in sis:\n",
    "                        for ei in eis:\n",
    "                            N = len(dsk_i.k.values[si:ei])\n",
    "                            if N < n_freqs_ni // 2:\n",
    "                                # Don't try to fit to less than half of \n",
    "                                # available points\n",
    "                                continue\n",
    "                            elif N < 5:\n",
    "                                # Don't fit to less than 5 data points\n",
    "                                continue\n",
    "                            # No NaNs allowed\n",
    "                            if np.any(np.isnan(dsk_i.k_spec.values.squeeze()[si:ei])):\n",
    "                                continue\n",
    "                            # Get diss. rate and R^2 of inertial subrange fit to \n",
    "                            # spectrum segment\n",
    "                            epsilon, r_squared, coeff = rpt.dissipation_rate(\n",
    "                                k=dsk_i.k.values[si:ei], \n",
    "                                spec=dsk_i.k_spec.values.squeeze()[si:ei], \n",
    "                                fit=fit)\n",
    "                            # Save R^2 and epsilon to dict\n",
    "                            rs_list.append(r_squared)\n",
    "                            eps_list.append(epsilon)\n",
    "                            si_list.append(si)\n",
    "                    # Find best fit from R^2 (highest) and fit length (longest)\n",
    "                    if len(rs_list) == 0:\n",
    "                        # No fits were made (too many NaNs)\n",
    "                        continue\n",
    "                    # Get index of max. R^2\n",
    "                    max_rsq_ind = np.argmax(rs_list)\n",
    "                    # Use dissipation rate estimate with highest R^2 value\n",
    "                    eps_est = eps_list[max_rsq_ind]\n",
    "                    # Use lowest k included in the best fit\n",
    "                    sim = si_list[max_rsq_ind]\n",
    "                    l = 2*np.pi / dsk_i.k.values[sim]\n",
    "                    # Compute eddy time scale (Eq. 6.11 of Pope, pg. 187)\n",
    "                    t_eddy = (l**2 / eps_est)**(1/3)\n",
    "                    # Compute advection time scale\n",
    "                    t_adv = l / U\n",
    "                    t_ratios[mid].append(t_eddy / t_adv)\n",
    "                    # Check t_eddy vs. t_adv criterion, only save spectrum\n",
    "                    # for later ensemble averaging if ratio high enough\n",
    "                    if (t_eddy / t_adv) < eddy_adv_ratio:\n",
    "                        # Ratio too low -> don't save spectrum\n",
    "                        continue\n",
    "                    # Estimate new k-spectrum and interpolate to pre-defined k axis\n",
    "                    dsk = rpt.k_spec_wavephase(w=wseg_ni, U=U, k_int=ki)\n",
    "                    # Add wave height and wave counter as variable\n",
    "                    dsk['Hw'] = ([], Hwi)\n",
    "                    dsk['wcnt'] = ([], wcnt)\n",
    "                    # Add time coordinate to spectrum dataset\n",
    "                    dsk = dsk.assign_coords(time=[t0seg])\n",
    "                    # Check highest allowable k\n",
    "                    maxk = np.min((2*np.pi/0.05, np.pi*fs/U))\n",
    "                    # Set to NaN all spectral elements with k > maxk\n",
    "                    dsk.k_spec.values = dsk.k_spec.where(dsk.k < maxk).values\n",
    "                    # Append to list\n",
    "                    dssi['q{}'.format(i+1)].append(dsk)\n",
    "\n",
    "            # Concatenate spectral datasets and save to netcdf\n",
    "            dss = {} # Datasets to concatenate\n",
    "            for n in range(1, ncyc+1):\n",
    "                # Check if nc file already saved\n",
    "                fn_kspec_nc = os.path.join(spec_ncdir, 'kspec_{}_{}_{}.nc'.format(\n",
    "                    mid, 'q{}'.format(n), t0b.strftime('%Y%m%d_%H%M')))\n",
    "                if os.path.isfile(fn_kspec_nc):\n",
    "                    dss['q{}'.format(n)] = xr.open_dataset(fn_kspec_nc, \n",
    "                                                           decode_cf=True)\n",
    "                else:\n",
    "                    if len(dssi['q{}'.format(n)]) > 0:\n",
    "                        dss['q{}'.format(n)] = xr.concat(\n",
    "                                dssi['q{}'.format(n)], dim='time')\n",
    "                        # Save to netcdf\n",
    "                        if not os.path.isfile(fn_kspec_nc):\n",
    "                            dss['q{}'.format(n)].to_netcdf(fn_kspec_nc)\n",
    "\n",
    "            # Dicts to store curve fit coeff.\n",
    "            rsqs = {} # R^2 value for best fit\n",
    "            coeffs = {} # Fit coefficient for best fit\n",
    "            sid = {} # Optimal start index for fit\n",
    "            eid = {} # Optimal end index for fit\n",
    "            diss_rate = {}\n",
    "            U_diss = {}\n",
    "            # Define figure filename\n",
    "            figdir_spec = os.path.join(figdir, 'turb_spec')\n",
    "            if not os.path.isdir(figdir_spec):\n",
    "                os.mkdir(figdir_spec)\n",
    "            fn_fig = os.path.join(figdir_spec, 'kspec_{}_{}_ubins_{}_fit_{}.png'.format(\n",
    "                mid, t0b.strftime('%Y%m%d_%H%M'), nbins, fit))\n",
    "            # Initialize test plot 1, k-spectra and -5/3 fits\n",
    "            if plot1 and not os.path.isfile(fn_fig):\n",
    "                # Define fit function\n",
    "                def fun(x, c):\n",
    "                    \"\"\"\n",
    "                    Standard curve fit to inertial subrange k^{-5/3}.\n",
    "                    \"\"\"\n",
    "                    return c * x ** (-5/3)\n",
    "                nrows = ncyc//4\n",
    "                fig, axes = plt.subplots(figsize=(7,2.5*nrows), \n",
    "                    ncols=4, nrows=nrows, sharex=True, sharey=True)\n",
    "                csu = ['C0', 'C1', 'C2', 'C3'] # Colors for plot\n",
    "            # Iterate over sub-sections\n",
    "            for n in range(1, ncyc+1):\n",
    "                # Iterate over U ranges defined by nbins\n",
    "                for ui in range(nbins):\n",
    "                    # Select correct dataset\n",
    "                    if f'q{n}' in dss.keys():\n",
    "                        dsi = dss['q{}'.format(n)].copy()\n",
    "                        # Select correct U range if using more than 1 U bin\n",
    "                        if nbins > 1:\n",
    "    #                         dsi = dsi.where(\n",
    "    #                             np.logical_and(dsi.U>=ubins[ui][0], dsi.U<ubins[ui][1]), \n",
    "    #                             drop=True,\n",
    "    #                             )\n",
    "                            dsi = dsi.isel(\n",
    "                                time=np.logical_and(\n",
    "                                    dsi.U>=ubins[ui][0], dsi.U<ubins[ui][1]))\n",
    "                            # Check that the selected U range has data\n",
    "                            if len(dsi.time) == 0:\n",
    "                                continue\n",
    "                        # Average spectra for wavenumbers with no more than maxnan \n",
    "                        # fraction of NaN values\n",
    "                        maxnan = 0.2 # Max. allowed fraction of NaNs per wavenumber\n",
    "                        good_inds = (dsi.k_spec.isnull().sum(dim='time')/len(dsi.time)<=maxnan)\n",
    "                        ps_mean = dsi.where(good_inds).mean(dim='time').k_spec.values\n",
    "                        # Also get corresponding U values\n",
    "                        U_mean = dsi.where(good_inds).U.mean(dim=['k', 'time']).item()\n",
    "                        # Lists to store curve fit coeff.\n",
    "                        rsqs_i = [] # R^2 value for best fit\n",
    "                        coeffs_i = [] # R^2 value for best fit\n",
    "                        sid_i = [] # Optimal start index for fit\n",
    "                        eid_i = [] # Optimal end index for fit\n",
    "                        npts = [] # Number of points to fit\n",
    "                        diss_i = [] # Dissipation rates\n",
    "                        U_i = [] # rms velocities\n",
    "                        # Iterate over different start/end freq. indices for fit\n",
    "                        # Disregard first (0-th) wavenumber\n",
    "                        sis = np.arange(1, n_int//2) # Start indices of fits\n",
    "                        eis = -np.arange(1, n_int//2)[::-1] # End inds of fits\n",
    "                        for si in sis:\n",
    "                            for ei in eis:\n",
    "                                N = len(ki[si:ei])\n",
    "                                if N < n_int // 2:\n",
    "                                    # Don't try to fit to less than half of \n",
    "                                    # available points\n",
    "                                    continue\n",
    "                                # No NaNs allowed\n",
    "                                if np.any(np.isnan(ps_mean[si:ei])):\n",
    "                                    continue\n",
    "                                # Get diss. rate and R^2 of inertial subrange fit to \n",
    "                                # spectrum segment\n",
    "                                epsilon, r_squared, coeff = rpt.dissipation_rate(\n",
    "                                    k=ki[si:ei], spec=ps_mean[si:ei], fit=fit)\n",
    "                                # Save to dict if R^2 high enough\n",
    "                                if r_squared >= r2_thresh:\n",
    "                                    rsqs_i.append(r_squared)\n",
    "                                    coeffs_i.append(coeff)\n",
    "                                    diss_i.append(epsilon)\n",
    "                                    sid_i.append(si)\n",
    "                                    eid_i.append(ei)\n",
    "                                    npts.append(N)\n",
    "                                    U_i.append(U_mean)\n",
    "                        # Find best fit from R^2 (highest) and fit length (longest)\n",
    "                        if len(rsqs_i) == 0:\n",
    "                            # No fits were made (too many NaNs)\n",
    "                            continue\n",
    "                        max_rsq_ind = np.argmax(rsqs_i)\n",
    "                        # Save parameters corresponding to max R^2\n",
    "                        diss_rate['q{}'.format(n)] = diss_i[max_rsq_ind]\n",
    "                        rsqs['q{}'.format(n)] = rsqs_i[max_rsq_ind]\n",
    "                        coeffs['q{}'.format(n)] = coeffs_i[max_rsq_ind]\n",
    "                        U_diss['q{}'.format(n)] = U_i[max_rsq_ind]\n",
    "                        sid['q{}'.format(n)] = sid_i[max_rsq_ind]\n",
    "                        eid['q{}'.format(n)] = eid_i[max_rsq_ind]\n",
    "                        # Also append to ensemble-avg list\n",
    "                        diss_subseg[mid]['{}/{}'.format(n, ncyc)].append(\n",
    "                            diss_i[max_rsq_ind])\n",
    "                        U_subseg[mid]['{}/{}'.format(n, ncyc)].append(\n",
    "                            U_i[max_rsq_ind])\n",
    "                    else:\n",
    "                        # Data not good enough for any dissipation rate estimates\n",
    "                        diss_rate['q{}'.format(n)] = np.nan\n",
    "                        rsqs['q{}'.format(n)] = np.nan\n",
    "                        coeffs['q{}'.format(n)] = np.nan\n",
    "                        U_diss['q{}'.format(n)] = np.nan\n",
    "                        # Also append to ensemble-avg list\n",
    "                        diss_subseg[mid]['{}/{}'.format(n, ncyc)].append(np.nan)\n",
    "                        U_subseg[mid]['{}/{}'.format(n, ncyc)].append(np.nan)\n",
    "                    # Plot spectra and -5/3 fits for each wave-phase \n",
    "                    # subsegment?\n",
    "                    if plot1 and not os.path.isfile(fn_fig):\n",
    "                        # Check is current segment has data to plot\n",
    "                        if 'q{}'.format(n) in sid.keys():\n",
    "                            si = sid_i[max_rsq_ind]\n",
    "                            ei = eid_i[max_rsq_ind]\n",
    "                            rsq = rsqs_i[max_rsq_ind]\n",
    "                            cf = coeffs_i[max_rsq_ind]\n",
    "                            diss = diss_i[max_rsq_ind]\n",
    "                            ax = axes.flat[n-1]\n",
    "                            dsi.k_spec.isel(k=slice(1, None)).mean(dim='time').plot(\n",
    "                                ax=ax, c=csu[ui], label='ubin {}'.format(ui+1))\n",
    "                            ax.loglog(ki[si:ei], fun(ki[si:ei], cf), \n",
    "                                linestyle='--', color='k')\n",
    "                            if nbins == 1:\n",
    "                                # Annotate R^2\n",
    "                                ax.annotate(r'$R^2$='+'{:.2f}'.format(rsq), xy=(0.05, 0.03), \n",
    "                                            xycoords='axes fraction', fontsize=9)\n",
    "                                # Annotate dissipation rate\n",
    "                                ax.annotate(r'$\\epsilon$='+'{:.2E}'.format(diss), \n",
    "                                            xy=(0.05, 0.11), xycoords='axes fraction', \n",
    "                                            fontsize=9)\n",
    "\n",
    "            if plot1 and not os.path.isfile(fn_fig):\n",
    "                # Labels & ticks\n",
    "                for ax in axes[1,:]:\n",
    "                    ax.set_xlabel(r'$k$ [rad/m]')\n",
    "                    ax.set_xlim([10, 200])\n",
    "                    ax.set_ylim([1e-7, 1e-3])\n",
    "                    ax.set_xticks([10, 100])\n",
    "                    # ax.get_xaxis().get_major_formatter().labelOnlyBase = False\n",
    "                    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "                for ax in axes[:,0]:\n",
    "                    ax.set_ylabel(r'$\\phi(k)$ [$(\\mathrm{m}^2\\mathrm{s}^{-2})$/(rad/m)]')\n",
    "                # Set log-log axes\n",
    "                for na, ax in enumerate(axes.flat):\n",
    "                    if ax.lines:\n",
    "                        # Legend if not empty plot (to avoid warnings)\n",
    "                        ax.legend(fontsize=8)\n",
    "\n",
    "                    ax.set_xscale('log')\n",
    "                    ax.set_yscale('log')\n",
    "                    # Annotate sub segment number\n",
    "                    ax.annotate('{}/{}'.format(na+1, ncyc), xy=(0.03, 0.88), \n",
    "                                xycoords='axes fraction', fontsize=14)\n",
    "                plt.suptitle('{} {}, # of U-bins: {}, fit: {}'.format(\n",
    "                    mid, t0b, nbins, fit))\n",
    "                plt.tight_layout()\n",
    "                # plt.show()\n",
    "                plt.savefig(fn_fig, bbox_inches='tight', dpi=300)\n",
    "                plt.close()\n",
    "            # Save burst-average dissipation and U_rms\n",
    "            dfbd[mid]['eps_phase'].loc[t0b] = np.array(list(diss_rate.values())).mean()\n",
    "            dfbd[mid]['eps_phase_std'].loc[t0b] = np.array(list(\n",
    "                diss_rate.values())).std()\n",
    "            dfbd[mid]['U_rms'].loc[t0b] = Urms\n",
    "            dfbd[mid]['depth'].loc[t0b] = h\n",
    "            # dfbu[mid].loc[t0b] = Urms / h\n",
    "        # Save to csv\n",
    "        dfbd[mid].index = dfbd[mid].index.rename('time')\n",
    "        dfbd[mid].to_csv(fn_csv_diss)\n",
    "    else:\n",
    "        dfbd[mid] = pd.read_csv(fn_csv_diss, parse_dates=['time']).set_index('time')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roxsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
